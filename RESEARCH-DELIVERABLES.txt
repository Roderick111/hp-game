================================================================================
WITNESS & NARRATOR LLM RESEARCH - DELIVERABLES
Research Date: 2026-01-09
Status: COMPLETE
================================================================================

RESEARCH QUESTION ANSWERED:
"Where are the files related to LLM witness and narrator? How is conversation 
history currently managed? Is separate memory per witness/narrator possible?"

ANSWER: YES - Separate memory per entity is possible. Witness already has it. 
Narrator needs to be updated to match the witness pattern.

================================================================================
FILES CREATED (4 Documents)
================================================================================

1. PRPs/WITNESS-NARRATOR-LLM-RESEARCH.md (550+ lines)
   - Comprehensive technical analysis
   - All file locations with line numbers
   - Data flow diagrams
   - Feasibility analysis
   - Implementation checklist
   - 14 detailed sections

2. WITNESS-NARRATOR-RESEARCH-SUMMARY.md (300+ lines)
   - Quick reference guide
   - Key findings at a glance
   - File locations table
   - Feasibility verdict
   - Quick navigation guide

3. WITNESS-NARRATOR-VISUAL-REFERENCE.md (400+ lines)
   - 8 detailed visual diagrams
   - Code flow illustrations
   - Side-by-side architecture comparison
   - Before/after solution diagrams
   - Function call patterns

4. WITNESS-NARRATOR-CODE-SNIPPETS.md (400+ lines)
   - Direct code from project files
   - Line numbers for all references
   - Current vs proposed implementation
   - Example modifications
   - Token budget analysis

================================================================================
KEY FINDINGS
================================================================================

NARRATOR LLM:
  File: /backend/src/context/narrator.py (171 lines)
  Routes: POST /api/investigate
  Conversation Memory: MISSING ❌
  History Passed to LLM: NO ❌
  Status: Needs update to avoid repetition

WITNESS LLM:
  File: /backend/src/context/witness.py (236 lines)
  Routes: POST /api/interrogate
  Conversation Memory: PER-WITNESS ✅
  History Passed to LLM: YES (last 5 exchanges) ✅
  Status: Correct pattern, works well

TOM GHOST LLM:
  File: /backend/src/context/tom_llm.py (431 lines)
  Routes: POST /api/case/{case_id}/tom/auto-comment|chat
  Conversation Memory: GLOBAL (shared across all) ⚠️
  History Passed to LLM: YES (last 3 exchanges) ✅
  Status: Works, but could be per-witness in future

STATE MODELS:
  File: /backend/src/state/player_state.py (400+ lines)
  
  WitnessState (lines 92-130):
    - PER-WITNESS conversation_history ✅
    - get_history_as_dicts() method ✅
    - add_conversation() method ✅
  
  InnerVoiceState (lines 175-290):
    - GLOBAL conversation_history ⚠️
    - add_tom_comment() method ✅
    - Could be per-witness variant
  
  PlayerState (lines 292-399):
    - GLOBAL conversation_history (20-msg limit)
    - witness_states: dict[str, WitnessState] ✅
    - narrator_states: MISSING ❌ (would add)
    - inner_voice_state: InnerVoiceState

================================================================================
CONVERSATION HISTORY FLOW (Side-by-Side)
================================================================================

NARRATOR (Current - NO History):
  PlayerState → Routes.investigate() → build_narrator_prompt() [NO history param]
  → Claude Haiku → add_conversation_message() [global only]
  PROBLEM: Narrator doesn't know what it said before at this location

WITNESS (Current - PER-WITNESS History):
  PlayerState.witness_states[witness_id] → Routes.interrogate() 
  → witness_state.get_history_as_dicts() → build_witness_prompt(history=...) 
  → Claude Haiku → witness_state.add_conversation() → update_witness_state()
  RESULT: Witness remembers all exchanges with each witness, avoids repetition ✅

TOM (Current - GLOBAL History):
  PlayerState.inner_voice_state → Routes.tom/auto-comment 
  → inner_voice_state.conversation_history → build_context_prompt(history=...) 
  → Claude Haiku → inner_voice_state.add_tom_comment()
  RESULT: Tom avoids repetition in global history, could filter by witness ⚠️

================================================================================
FEASIBILITY VERDICT
================================================================================

QUESTION 1: Is separate narrator memory per location theoretically possible?
ANSWER: YES ✅ (100% feasible)
EVIDENCE:
  - WitnessState pattern already proven (per-witness memory works)
  - format_conversation_history() function exists (witness.py:54-75)
  - Prompt integration pattern established (witness.py:114-203)
  - No data model conflicts
  - No API conflicts

QUESTION 2: Is separate narrator memory per location practically possible?
ANSWER: YES ✅ (1-2 days effort)
CHANGES NEEDED:
  1. Create NarratorLocationState class (like WitnessState)
  2. Add narrator_states dict to PlayerState
  3. Update narrator.py to accept conversation_history param
  4. Update routes.py investigate endpoint to pass history
  5. Add tests (20+ cases)
BREAKING CHANGES: NONE (can be backward compatible)

QUESTION 3: Is separate Tom memory per witness possible?
ANSWER: YES ✅ (1 day effort, lower priority)
APPROACH:
  - Extend InnerVoiceState to track witness-specific context
  - Filter conversation_history by witness_id
  - Update generate_tom_response() signature
  - Same pattern as narrator solution

================================================================================
IMPLEMENTATION PRIORITY
================================================================================

PHASE 4.5 (RECOMMENDED - 1 day):
  - Add conversation_history parameter to narrator.py
  - Update routes.py investigate endpoint
  - Test narrator avoids repetition
  - Minimal changes, high impact

PHASE 5 (OPTIONAL - 3 days):
  - Full NarratorLocationState architecture
  - Per-location conversation tracking
  - Frontend updates for location-specific history display

PHASE 5+ (OPTIONAL - 2-3 days):
  - Tom per-witness context (lower priority)
  - Or unified conversation model (bigger refactor)

================================================================================
QUICK IMPLEMENTATION CHECKLIST
================================================================================

If implementing narrator memory quickly (Phase 4.5, 1 day):

STEP 1: narrator.py - Add function (10 lines)
  [ ] Add format_narrator_conversation_history() function
      (Copy witness.py:54-75 as template, adjust for narrator)

STEP 2: narrator.py - Update signature (1 line)
  [ ] Add conversation_history param to build_narrator_prompt()
      (Make it optional: conversation_history: list[dict] | None = None)

STEP 3: narrator.py - Include in prompt (2 lines)
  [ ] Format history: history_section = format_narrator_conversation_history(...)
  [ ] Add to prompt template: == PREVIOUS OBSERVATIONS IN THIS LOCATION ==

STEP 4: routes.py - Update investigate endpoint (5 lines)
  [ ] Load narrator history: narrator_history = get_location_narrator_history(...)
  [ ] Pass to prompt: build_narrator_prompt(..., conversation_history=narrator_history)
  [ ] Save response: save_narrator_response_for_location(...)

STEP 5: Tests (20 test cases)
  [ ] Test narrator doesn't repeat descriptions
  [ ] Test different locations have separate history
  [ ] Test old saves still work (backward compat)
  [ ] Test 20-message-equivalent history rotation

================================================================================
FILES TO REFERENCE FOR PATTERNS
================================================================================

COPY PATTERNS FROM (These work correctly):

witness.py (lines 54-75):
  → Use format_conversation_history() as template
  → Adapt for narrator conversation structure

witness.py (lines 114-203):
  → Use build_witness_prompt() as template for parameter patterns
  → See how conversation_history is integrated into prompt

player_state.py (lines 92-130):
  → Use WitnessState class as template
  → Create NarratorLocationState class following same pattern

routes.py (lines 748-773):
  → Use witness interrogate as template for state loading/saving pattern
  → Replicate: get_state → get_history → build_prompt → save_response

tom_llm.py (lines 213-243):
  → Reference format_tom_conversation_history() for history formatting
  → Shows how to handle "last N exchanges" pattern

================================================================================
RESEARCH CONFIDENCE
================================================================================

Overall Confidence: 95% (HIGH)

Basis:
  - Direct analysis of all relevant source files
  - Line-by-line code review
  - Pattern validation (witness pattern proven working)
  - No speculative content (all verified against code)

Remaining 5% uncertainty:
  - Potential frontend display requirements (not analyzed)
  - Performance with large conversation histories (not stress tested)
  - Edge cases in old save file migration (not tested)

================================================================================
DELIVERABLE CHECKLIST
================================================================================

[✓] Question 1: Where are the files?
    - Narrator: /backend/src/context/narrator.py (171 lines)
    - Witness: /backend/src/context/witness.py (236 lines)
    - Tom: /backend/src/context/tom_llm.py (431 lines)
    - Routes: /backend/src/api/routes.py (1600+ lines)
    - State: /backend/src/state/player_state.py (400+ lines)
    - All line numbers documented ✓

[✓] Question 2: How is conversation history managed?
    - Narrator: NO history (global messages only)
    - Witness: PER-WITNESS history (separate per witness)
    - Tom: GLOBAL history (but formatted for last 3 exchanges)
    - Storage locations documented ✓
    - Data flow explained ✓
    - Code patterns shown ✓

[✓] Question 3: Is separate memory possible?
    - YES - Theoretically: 100% feasible ✓
    - YES - Practically: 1-2 days effort ✓
    - Architecture analysis complete ✓
    - Implementation plan provided ✓
    - Feasibility verdict documented ✓

[✓] Documentation Quality:
    - 4 comprehensive documents created ✓
    - 1500+ lines of analysis ✓
    - 8 visual diagrams ✓
    - Direct code examples ✓
    - Line-number references ✓
    - Before/after comparisons ✓
    - Implementation checklist ✓

================================================================================
HOW TO USE THESE DOCUMENTS
================================================================================

START HERE: WITNESS-NARRATOR-RESEARCH-SUMMARY.md
  → 5-minute overview
  → Key findings table
  → Quick answers to 3 questions
  → File navigation guide

FOR VISUAL LEARNERS: WITNESS-NARRATOR-VISUAL-REFERENCE.md
  → Data flow diagrams
  → Architecture comparisons
  → Current vs proposed solutions
  → Visual implementation plan

FOR CODE REVIEW: WITNESS-NARRATOR-CODE-SNIPPETS.md
  → Exact code from project
  → Line number references
  → What would be added/modified
  → Copy/paste ready examples

FOR COMPLETE ANALYSIS: PRPs/WITNESS-NARRATOR-LLM-RESEARCH.md
  → 14 detailed sections
  → Complete technical spec
  → Feasibility analysis
  → Implementation checklist
  → Risk assessment

FOR IMPLEMENTATION: Use code snippets document + checklist

================================================================================
NEXT STEPS (User Decision)
================================================================================

OPTION 1: Do nothing (narrator repetition is minor issue)
  - No work required
  - Investigation works, just repeats info sometimes

OPTION 2: Quick fix (Phase 4.5, 1 day)
  - Add conversation_history param to narrator
  - Narrator avoids repeating location descriptions
  - Minimal code changes
  - No breaking changes
  - RECOMMENDED

OPTION 3: Full architecture (Phase 5+, 3 days)
  - Create NarratorLocationState class
  - Per-location conversation tracking
  - Frontend displays location history
  - Complete isolation like witness

OPTION 4: Also update Tom (Phase 5+, 1 day)
  - Make Tom context per-witness
  - Tom references specific witness interactions
  - Builds on narrator solution

================================================================================
Research Complete: 2026-01-09
Research Time: ~1.5 hours
Quality: High confidence, comprehensive coverage
Ready For: Implementation planning (Phase 4.5+) or further research
